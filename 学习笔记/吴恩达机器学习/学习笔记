1、回归：回归，指研究一组随机变量(Y1 ，Y2 ，…，Yi)和另一组(X1，X2，…，Xk)变量之间关系的统计分析方法，又称多重回归分析。
    回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。当函数为参数未知的线性函数时，称为线性回归分析模型；
    当函数为参数未知的非线性函数时，称为非线性回归分析模型。当自变量个数大于1时称为多元回归，当因变量个数大于1时称为多重回归。

2、回归分析的主要内容有以下：
    ①从一组数据出发，确定某些变量之间的定量关系式；即建立数学模型并估计未知参数。通常用最小二乘法。
    ②检验这些关系式的可信任程度。
    ③在多个自变量影响一个因变量的关系中，判断自变量的影响是否显著，并将影响显著的选入模型中，剔除不显著的变量。通常用逐步回归、向前回归和向后回归等方法。
    ④利用所求的关系式对某一过程进行预测或控制。

3、回归主要的种类有：线性回归、曲线回归、二元logistic回归、多元logistic回归。

4、在大数据分析中，回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。

5、Linear Regression线性回归：线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。
   Logistic Regression逻辑回归：逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。
                              在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差。
    Polynomial Regression多项式回归：对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。

6、最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。
    最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。建立如下规则：被选择的参数，使算出的函数曲线与观测值之差的平方和最小。

7、迭代:重复反馈过程的活动，其目的通常是为了逼近所需目标或结果。每一次对过程的重复称为一次“迭代”，而每一次迭代得到的结果会作为下一次迭代的初始值。
    重复执行一系列运算步骤，从前面的量依次求出后面的量的过程。此过程的每一次结果，都是由对前一次所得结果施行相同的运算步骤得到的。
    对计算机特定程序中需要反复执行的子程序*(一组指令)，进行一次重复，即重复执行程序中的循环，直到满足某条件为止，亦称为迭代。
    迭代法也称辗转法，是一种不断用变量的旧值递推新值的过程，跟迭代法相对应的是直接法(或者称为一次解法)，即一次性解决问题。迭代算法是用计算机解决问题的一种基本方法，它利用计算机运算速度快、适合做重复性操作的特点，
    让计算机对一组指令(或一定步骤)进行重复执行，在每次执行这组指令(或这些步骤)时，都从变量的原值推出它的一个新值，迭代法又分为精确迭代和近似迭代。比较典型的迭代法如“二分法”和"牛顿迭代法”属于近似迭代法。
    最常见的迭代法是牛顿法。其他还包括最速下降法、共轭迭代法、变尺度迭代法、最小二乘法、线性规划、非线性规划、单纯型法、惩罚函数法、斜率投影法、遗传算法、模拟退火等等。
    一步一步逐渐精确。逻辑上：多次使用同一算法。
    在计算机科学中，迭代是程序中对一组指令（或一定步骤）的重复。它既可以被用作通用的术语（与“重复”同义），也可以用来描述一种特定形式的具有可变状态的重复。
    在第一种意义下，递归是迭代的一个例子，但是通常使用一种递归式的表达。比如用0!=1，n!=n*(n-1)!来表示阶乘。而迭代通常不是这样写的。
    而在第二种（更严格的）意义下，迭代描述了在指令式编程语言中使用的编程风格。与之形成对比的是递归，它更偏向于声明式的风格。
    由于数值迭代是逐步逼近最优点而获得近似解的，它无限地接近于最优点却又不是理论上的最优点，所以就需要考虑在什么样的条件下才终止迭代，获得一个足够精度的近似极小点，这一条件就是迭代计算的终止准则。
    终止准则：
        (1) 点距准则：当相邻两迭代点X(k)、X(k+1)之间的距离已达到充分小时。
        (2)函数下降量准则：当相邻两迭代点X(k)、X(k+1)的目标函数值的下降量已达到充分小时。
        (3)梯度准则：当目标函数在迭代点的梯度已达到充分小时，迭代终止。
        在优化设计中，一般只要满足以上终止准则之一，则可认为设计点收敛于极值点。
    三种常用的迭代搜索优化方法：梯度下降法、牛顿方法、坐标上升的方法（在支持向量机部分引入的，为了解决soft margin SVM的优化问题）
        梯度下降法：在解决多维度的优化问题中最常见的局部最优问题。究其原因是梯度下降法的搜索准则所致，按照梯度的负方向搜索，一味追求网络误差或能量函数的降低，使得搜索只具有“下山”的能力，
            而不具备“爬山”的能力。所谓“爬山”的能力，就是当搜索陷入局部最优时，还能具备一定的“翻山越岭”的能力，能够从局部最优中逃出来，继续搜索全局最优。
        牛顿法则是通过分析极大和极小值处曲线的特性，通过求导，并使导数为0，构造典型的f(X)=0f(X)=0的优化形式，每一步都从该点处的切线位置与XX轴（或平面）相交的处的X作为下一次迭代的搜索位置的X坐标（对应的yy可以通过f(X)f(X)求得）。
            通常情况下牛顿法收敛速度比梯度下降方法要快。
        坐标上升的方法，这个优化搜索方法是在支持向量机部分引入的，为了解决soft margin SVM的优化问题。 
    梯度下降与牛顿方法是两种非常常用的迭代优化方法，主要的思想就是通过迭代，一步一步地逼近最优解。

8、递归，就是在函数内部又去调用自己。递归实际上就是将规模为n的问题降价为n-1的问题进行求解。也就是去找n和n-1之间的关系。

9、递推：找到数学规律：通过公式计算到下一项的值，一直到我们要的结果为止。例如：兔子产子：通过前俩项得到下一项。递推分为顺推和逆推。

10、穷举：遇到一个问题，找不到更好的解决办法，（找不到数学公式或者规律）时，使用“最笨”的办法，利用计算机计算速度快的特点，将所有可能性全部列出来并将我们想要得到的结果记录下来。
    穷举方法的特点：是算法简单，相应的程序也简单，但是计算量往往很大。但是计算机的优势就是运算速度快，所以此算法可以扬长避短，往往可以取得不错的效果。
    案例：1)有一个三位数，个位数字比百位数字大，而百位数字又比十位数字大，并且各位数字之和等于各位数字相乘之积，求此三位数。2）百钱白鸡问题。

11、交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),
    首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。
    用交叉验证的目的是为了得到可靠稳定的模型。在建立PCR 或PLS 模型时，一个很重要的因素是取多少个主成分的问题。用cross validation 校验每个主成分下的PRESS值，选择PRESS值小的主成分数。或PRESS值不再变小时的主成分数。

12、正则化是一种回归的形式，它将系数估计（coefficient estimate）朝零的方向进行约束、调整或缩小。也就是说，正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。

